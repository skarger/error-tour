Hi everyone, I'm Stephen Karger. Thank you for attending.

So this material will range about 45 minutes,
so there will be a little time at the end if people want to stick around for questions or comments.

So let's get started.

OK, before I get into the specific topic this talk is all about
I want start out with a bigger question.

NS
When it comes to software, what is quality?

What does it actually mean for software to be high quality?

If you think you've built quality software in the past, what did you do?
What details made it quality?

So I've come up with an answer that I like,
which is that quality software has the following traits:

Build Out 1

* First, does the software provide a nice user interface?
    Does it look good, is it intuitive, does it make it easy to do what you need to do?
* Another trait is performance. If it's responsive and fast then it's higher quality.
* Next, does the software do things correctly?
    This one is context dependent, like say you build a weather forecasting program and you
    use floating point numbers, you'll have rounding errors but it can still be
    reasonably correct by the expectations of the program. Vs. if you build
    a payments app with floating point then it will be incorrect in a way that bothers people.
    A hallmark of good quality software is that it's gone to the trouble of
    sorting out appropriate definitions of correctness for its situation, and implemented them.
* And a final quality trait, does the software handle errors gracefully?
    Programs encounter errors for external reasons like network issues or unexpected user input,
    and sometimes they have plain old bugs in their internal logic. It happens.
    Either way the software may still handle that error well,
    and that makes a real difference in the user's experience.

Build Out 2
One reason I like this definition is that it helps distinguish
the meaning of quality from the software's features.
Surely there is some overlap, like sometimes you'll hear people say that performance is a feature,
and I truly understand that point,
but it's different from what we usually understand as domain features.

Also, this helps shed light on why software quality is challenging to attain,
because it is domain features that our business stakeholders usually request from us.
Features are the front line of making sales.
We have PMs and eng management putting a lot of coordinated effort into planning out feature development.
Whereas pulling off quality requires a lot of initiative from individual engineers.
Certainly management here has been willing to sponsor multiple projects that help drive quality,
but it's still something that tends to be more of a grassroots effort than our feature roadmap.

And this talk itself is a small example of trying to level-up on quality.
It's a deep dive on that last trait of error handling.

Handling errors in software is hardly a new topic,
it's more like a plain old day-to-day software engineering concern.
But I also believe that it's a deceptive topic, because despite its ordinariness it's not solved in practice.

One of the ongoing challenges we have in software development is to keep it simple, just solve today's problem,
because so often, over-engineering leads to complexity and maintainability problems.

However something I've noticed is that the status quo for handling errors is under-engineering.
Across my software engineering life it's been something we deal with case-by-case, in an ad-hoc and incomplete way.

But I think it's practically the definition of a low-hanging fruit,
where being slightly more deliberate with error handling
is a direct way to produce higher quality software.

So the aim of this talk is to get into these details:
* What are the actual goals when it comes to handling errors in software?
* What tools do we have available and what are their tradeoffs?
* And what is a solid overall strategy to apply in the applications we are building?

NS
OK then. Here is the Enhanced Content UI in Salsify.
I've opened this publish dropdown in the top-right and it's working as intended,
showing me a collection of retailers where I can send my Enhanced Content.

NS
But in the next slide, this is a real bug that we had.
I click the Publish dropdown, and I just see this empty box.

By the way this bug was never reported to our dev team.
I only found it because I'd noticed it in Bugsnag a few times and investigated.

Well if I open the developer tools I can see that the API request
to load the retailers returned a 503, and that's why it couldn't populate the list.

But imagine the end-user's experience here. They see this empty box.
It's confusing, it's not truly evident that an error occurred.
A user might think they missed some setup step, or maybe
they think they just don't understand how the app works.

NS
Another example on this next slide, I've just used that panel on the right-hand side to
update my Enhanced Content layout to reference an Image property.

If I open dev tools though I can see that when I set that value,
the client tried to save the layout, but the API returned a 500.
So even though the UI looks fine, if I happened to reload the page
I'd discover that it didn't save my work.

And I want to emphasize that for end users the experience of errors is very frequently like this.
The software behaves in a way that seems off, but it's subtle. It's hard to understand cause and effect.
Ultimately that leads to lower confidence in the software.

NS
All right then what would we want to happen for software errors?
Build out 1
we don't want them to happen in the first place!
Clearly, preventing errors is preferable to dealing with them at runtime.

And we employ several techniques to try to prevent errors.
Build out 2
Testing, type checking, and code review are all helpful for preventing errors.

NS
But we all know our software still has errors in production.
So what should we do?

Over time we learn about bugs and we fix them,
say by supporting a case we hadn't known about before,
or telling the user to correct their input,
or adding a null check before we try to execute some logic.

Build out
And this process of widening the range of states that the program can handle,
so that errors become non-errors,
is one of our day-to-day tasks in software maintenance.

But I want to point out that these techniques are before-the-fact and after-the-fact.
When we write tests that guard against errors, by definition errors we have thought about, and we're preventing them ahead of time.
And we react to runtime errors that have happened in production by updating the code so they won't happen again.
When we apply these techniques we're using knowledge that we already have.

Importantly though, there's also category of errors that we don't know about concretely.
Our application has a code path where an error can happen,
but we're not aware of that right now,
or sometimes we are aware but we assume it's very low probability.

In other words, a guiding idea here is that error handling design needs to confront uncertainty.
When we think about how to handle runtime errors, we're designing for situations
that we don't have full knowledge about at this moment.

So in the remainder of this talk we're going to be deeper in the weeds, looking at code,
but keep this main idea in mind:

That error handling concerns situations that we do not fully anticipate.
It follows that the first principle behind error design is communication, because we can't fully automate the response to something that's unexpected.

A good error design needs to provide guidance for
    how do we inform end users that something went wrong.
    but also:
        how do we inform customer support,
        how do we inform PMs,
        and how do we inform ourselves?


NS
Now I want to talk about history.

The C language emerged in 1972 and it influenced a lot of what happened over the few couple decades.

NS
In C error handling conventionally happened via return values.

In this example C program we're making a system call to open a file.

Build Out 1
Opening the file returns a file pointer, and then we're trying to write to the open file.

Build Out 2
But the file doesn't exist. Writing to that non-existent file results in a segfault
and the program is killed by the operating system.

NS
So in the next slide I've fixed that bug.

Build Out 1
Now I have logic to only write to the file if the returned file pointer is not NULL.

Build Out 2
The program continues without crashing.

This works. But there are two problems. First, it's mashing the error signaling into the return value.
The function is supposed to return a file pointer but it might
give us something we cannot use as a file pointer.
Second and more importantly, in C nothing forces you check that return value.
Your program can keep barreling on.
And failing to check return values for errors is a very common cause of bugs,
and those bugs can be confusing, because they may manifest somewhere far downstream
of the original problem and cause the program to suddenly crash,
or write corrupt data, or have a security vulnerability.

NS
So then there was a stretch in the 80s and 90s where some new languages came out that
have become mainstream in commercial software development.
These languages shown here all brought a concept of Exceptions.

NS
I'm going to take a minute to review the basics,
starting with this example in Java.
In this example I want to parse some strings into integers.
It works for the string 1000 but then throws an exception for the string A grand.
And this shows the general form of Exceptions:
Try running some code, if any of the statements fail,
then immediately jump to the catch block, and execute that other code instead.
And then, regardless of what happened, execute the finally block.

NS
Next we see an example in Ruby, same exact concept, just using the words begin rescue ensure
instead of try catch finally.
<pause>

NS
And then we have a JavaScript example. An interesting point here, unlike Java,
JavaScript's parseInt function does not throw an exception when it fails. Instead it returns the value NaN.
I'll say more about that design later.
But for the sake of the example I had to check for NaN and then explicitly threw an exception.
But it still supports the same try-catch-finally construct where it uses Exceptions to signal errors.

So Exceptions were a way of preventing the problem in C where the programmer forgets to check return values.
Because exceptions forcibly re-route the code execution.

NS
But even though that's helpful it has a dark side.
Exceptions introduce implicit control flow, sort of like a GOTO statement, where they make code abruptly jump.

Here we have function a who calls b who calls c,
which is a little more like a real program
where we have a deep call stack,
which often even spans multiple files.

We don't always have that compact try/catch or begin/rescue
where you can see everything at a glance.
The error handling code may live far away from whatever throws the exception.

This can make program logic hard to reason about, because when you are reading the exception handling code somewhere in the program,
it's not always obvious what the context is.
What are the possible circumstances where
we'd be running this handler code,
and what details do we need to clean up?
Will we even have the references in scope that we would need
in order to clean up?
Is it important to re-raise?
When you have error handlers layered across your code base it's difficult
to verify that they all do the right thing.

And many software engineers out there see Exceptions as an overused technique.
It's too common to use them for control flow, and it becomes difficult to handle exceptions safely.


NS
In fact some people view these problems with Exceptions as unacceptable.

Build Out
A couple recent languages that have become mainstream decided not to have exceptions at all.
They went back to signalling errors with return values, like C, just with some twists.

NS
In Go, the big change compared to C is that functions return two distinct values,
with the error value in the second position,
so return values and errors are not overloaded.
(pause)
And this gets rid of the control flow jumps. But now you are free to ignore the error value again.

NS
In Rust, they moved the problem into the type system.
(pause)
The standard approach for functions that can fail is to wrap the return value in a Result type.
A Result data type has two variants, Ok or Err,
and the Rust type checker forces you to cover those possibilities.
So Rust doesn't have Exceptions, but the compiler prevents you from forgetting to handle errors.

This style of wrapping a return value in Result type supports a functional style of programming,
because instead of throwing an exception as this side effect of your function call,
you're just passing data in and getting data out.

NS
I'll show a more involved example of that next.
On this first slide I've written these three helper functions.
First the parse_number function tries to convert a string to an integer and returns a Result type,
where the Err value is a ParseIntError,
ensure_positive checks if an integer is greater than 0 and returns a Result type,
where the Err value is a String,
and double just multiplies a number by 2 and returns a plain number.
<pause>

NS
So then in the next slide I have a full demo using these functions.
The main program calls the two_track_computation function and passes it different strings.
The reason I've called it two_track_computation will be clear in a moment.

Looking at two_track_computation, it starts out by calling parse_number,
and the Result value flows into map_err which is just there
to adjust the type of Err value from a ParseIntError to a string,
and then it flows into the ensure_positive function, then it tries to
double the result, and finally it produces a string value depending on
whether the Result was Ok or Err.

The details of Rust aren't important here, the point is to see the pattern.
The data is flowing in a linear way from function to function,
with errors at any point conveyed inside these values of data type Result.
There are no Exceptions that cause the control flow to jump. Data-in, Data-out.

NS
This style is not unique to Rust,
in 2014 an F# programmer named Scott Wlaschin
wrote a blog post naming this style Railway Oriented Programming.
It's where data flows through your program
like it's on a railroad track, and as long as it's successful it keeps rolling along that success track,
but if it encounters an error at any point it switches to the error track.


NS
And you can string together a functional pipeline this way.
So this can be easier to follow than the control flow jumps you get with exceptions.

Now we can adopt a Railway Oriented style in Ruby and JavaScript,
where we represent errors as data that we return from functions.
And then using TypeScript for example you can even enforce
that the code handles the success and error variants.

NS
But there are some tradeoffs to consider about that. I'll mention a few of them.

One place where representing errors-as-data fits well is for errors that are part of the program's domain.
If encountering this error is a normal event that you expect to happen sometimes as this program runs,
then it's nice to process that error as plain old data, rather than using exceptions as a special construct.

That's really a way of re-stating what I mentioned earlier that a core role of
error handling is dealing with situations we do not expect. But now I'll make that more
precise by saying that it's actually Exceptions that fit best with situations we do not expect.
When something happens that really is outside the circumstances this program is intended to support,
exceptions are a nice way to just bail out immediately.

I will say that it can be fuzzy to say decide something is a normal expected problem or if it's exceptional.
An example of that which comes up at Salsify is when we make HTTP requests
between our backend services. It's well within the domain of HTTP in general
that you can receive a 404 or a 500 response, they're not really exceptional.
But when we're making inter-service requests,
getting those response statuses is usually not part of our application's domain,
because the HTTP request is just an implementation detail of our service architecture, it happens to be how we're loading data.
If we don't get a 200-level response it means something has gone wrong that is not a domain-level problem.
If the error response is a transient network error then we can retry,
but otherwise the application just has to bail out at that point.
So raising an exception in the calling service when we see that 404 response actually is a reasonable reaction.

NS
Then another tradeoff to consider with errors-as-data is loose coupling.
Thinking again of a web API, when it needs to return an error, how do we do it?
We're obligated to put the error on the wire.
We need to transmit it to a different computer running a different program.
So we send the error as data, maybe an HTTP response with status code 500 or 404,
and maybe we send a JSON payload with an errors field.

When we have two programs communicating remotely,
there's this imperative to make them work with looser coupling.

But even inside one program it may be useful to apply that lesson from web APIs,
and pass pure data around in our program to promote modularity.
If we want to have a clear boundary in our program, we can ask:
Will this work if between these two sub-systems I only can pass data?
If I were to extract these two parts into separate programs,
would it work to communicate over a web API or a message queue?
And could they effectively signal error situations to each other?

If it's desirable for those parts of the program to be loosely coupled,
then one of the steps toward that is to communicate errors as data.

NS
Another practical tradeoff between exceptions and errors-as-data is
Are you working in a platform that has exceptions?

Build Out
So Clojure is a language designed to run on the JVM and have seamless interop with Java.

And with TypeScript, yes we can use the type-checker to support static enforcement of Result types
in a similar style to Rust, but TypeScript is meant to integrate directly into JavaScript.

NS
You can see in this Clojure example it's calling Java to try to parse the string to an integer,
it calls the Java parseInt function to do that,
which it throws the exact same Java exception as in the earlier example.

So even though Clojure is functional language tha puts heavy emphasis on passing data
through your program, as a practical matter Java throws exceptions so Clojure has to work with that.

And JavaScript is the same way, even using TypeScript you do need to deal with exceptions anyway.

So when we have situations where you could go either way
between returning an error as data and throwing an exception, there is a pragmatic argument for just doing it in the standard way of the underlying platform.


NS
Lastly another very good reason to use exceptions is that it's a way
to make them visible to us. With exceptions, we have infrastructure in place to report them to Bugsnag.

A key challenge for us as developers is that errors are hard to see.
They only happen infrequently, hopefully, and they often only come out in real-world usage.
So mentally it's easy discount their importance.

Sometimes we'll realize there's a potential for an error, but we dismiss it as an "edge case".

NS
Calling something an edge case is like we have this square representing all the cases,
and most are the yellow in the middle there,
but there's a thin edge of cases around the outside that we're not covering.

But you need error reporting to understand the size of the edge.

NS
Is it more like this? And if so we really we ought to deal with it.

NS
In one of my previous jobs we had a situation where most of the time our cases looked like this,
but on the two days of the year when the times changed for daylight savings time,

NS
the edge was like this and we would have a disaster of sorting
out customer problems for the next few days.
But then things would settle down and it would become hard to prioritize fixing it.

In general you really need records of errors to understand
whether they are a priority.
Pragmatically, reporting exceptions to Bugsnag is the way we can accomplish that.

NS
For the back half of this talk I want to get into some more Salsify-specific details.
At the beginning I was talking about doing the more deliberate design for error handling,
but the truth is we already do that to some degree in our shared libraries.

NS
Starting out on the backend, here's a snippet from the SalsifyGraphQLServer ruby gem
which I've shortened to show on the slide.
For every GraphQL request, the BaseController will catch exceptions and report them to Bugsnag.
Then it translates the error into an API response following GraphQL conventions for an error payload.

There is more to it than that, the real code handles more details, but this is the general idea.
(pause to look at code)

NS
For background jobs, Salsify has an extension to Delayed Job.
It has a similar approach of catching errors and reporting them to Bugsnag, and that happens at the top level where the job
is kicked off.
(pause to look at code)

So if you're using these libraries then you automatically have this error handling framework.
And this is a very helpful default, but by itself it's not enough to attain high quality error handling.

What I've shown so far is backend only. But once we translate the error into an API response payload, then what?
Or in the case of a background job, once we update a database row to record that this action failed, then what?

We still need to communicate to the end user.

Making that final step, connecting the dots on the client side, really improves quality a lot.
That may be an obvious point but in real world development it's easy for it to fall through the cracks.

NS
That said, just as with the backend libraries, we already do have some built-in default error handling within the main Salsify client app.

What the frontend error handler service provides is when an Ember route tries to load data from the API,
but the API request returns an error status like a 500,
the client will flash an error message in a bar near the top of the screen.
It's a bit generic but at least it's providing feedback to the end-user that something went wrong.

NS
But let me show you another real example.

In this video you'll see I'm looking at my list of Enhanced Content layouts.
Then you'll see I'm going click one of the layout rows and then click the button to copy it.
But under the hood, the API request to load that layout is going to fail. You'll see that below in my network tab.

Let's watch...

So click, copy, nothing happens, no feedback.

And also remember those examples I showed at the beginning
with the Enhanced Content builder where the API returned errors,
but the end user had no signal in the UI.

So, in theory Salsify has this built-in feedback in the UI for failed API requests,
but in practice it's not always working.

Now in that video, the reason doesn't work is bit tricky.
The problem is that the scope for the error message is set to the copy modal,
but that modal isn't visible when the error occurs.
That's why you don't see the flash message.

On one hand you could blame the EC engine for this, because it has an error in its error handler.
But when we're working on an Ember engine like this, in a normal development workflow we can't see that error behavior,
because it's provided by the main app in dandelion. The only way to see it is to package the engine into dandelion
and test end-to-end, including forcing the backend API server to return a 500.
The fact is that problem has existed for awhile but I only discovered it because I was preparing this tech talk.

Also with our current setup, it's difficult to write an automated test
in the Enhanced Content engine that realistically exercises the UI behavior for a failed API request,
again because the error message bar lives in dandelion.

This is a place where the micro-frontends initiative should help, because a micro-app will
be able to manage more of its own UI, including its error display, so we won't have the same pitfalls as with an engine.

Still though, I'd say the current dandelion client has the right overall idea to have a default
behavior where if an API request fails, let's at least provide some visual feedback.

Because it ultimately doesn't matter whether the server indicates errors in a well-formatted way,
if we don't carry that through end-to-end so that the client communicates the problem to the user.

By the way in this example I focused on synchronous web requests but communication of background job failures
might be even more important for a lot of Salsify features.

As I showed before we have the backend error reporting extension for DelayedJob,
but for actually communicating background job failures to end users,
that's an example where the status quo is to figure it out ad hoc.
Usually, each time we add a new background job, the approach for handling failures isn't covered.
Then over time we see failures in production and it's like OK now we need to devise some UX to handle this case.
And we're starting out with basic questions like, what even screen do we display the error on?

So one actionable takeaway here is every time we add a new background job, consider how to communicate when it fails, and then verify that it works.
Similarly, for any API request the client makes, a good baseline for failures is verify that it flashes an error message bar in the style of the main dandelion client.

NS
Beyond server errors, I want to cover errors in the client itself,
for example in the logic of an Ember component.

NS
In this slide, I have a catch block set up that handles when I call this undefined function named badFunc.

Build Out 1

But there's no catch block lower down when I call anotherBadFunc.

When a JS runtime error happens but it's not handled by any catch block,
the browser will fire the window.onerror event.

Build Out 2

That can be configured to run a function as a last resort error handler.


NS
For rejected promises, similar idea but slightly different.

Build Out
Here I have a promise that will reject because it tries to parse HTML as JSON.
But in this code there's no catch chained onto the promise.

NS
In this case the browser will fire the unhandledrejection event.

Like window.onerror, this can be configured as a last resort handler for rejected promises.

In fact, the basic way to configure Bugsnag on the browser side is to set up window.onerror and unhandledrejection
to report errors to Bugsnag.

NS
In our case though, Ember provides a function, Ember.onerror, that's meant as the backstop for unhandled errors in Ember apps.

Build Out
And then Salsify's error-reporting library abstracts over Ember.onerror.
Ultimately, the error-reporting library configures reporting to Bugsnag.
That happens once in Dandelion. That one-time setup then applies to our mounted engines.

But it does mean that in your engine you can't use these top-level error events yourself.
In particular you can't use them as a way to communicate problems to end users, in addition to Bugsnag.
This is another place where the micro-frontends project will enable improvements.

NS
Now let me make this more visual with an example of a client error.
Here we're looking at the list of Enhanced Content layouts again.

Notice the rows of the table. They're Ember components.

Build Out
So we have a component representing the table row.
And then we have a couple components nested inside of it

Build Out
one that displays what retailers you've published this layout to,
Build Out
and another component that shows the status of the layout.

So imagine that that "Most Recent Status" component throws an exception, what's going to happen?

NS
Well the overall layout table still renders, but because the status component errored, everything
after it is gone. You can see there's no status, but also the rows after the first one aren't showing.

Build Out
Also, unexpectedly the error thrown by that component
also causes this Amazon quota bar up at the top to break as well, where now it's not showing counts anymore.
That's a somewhat arbitrary side effect of the fact that that the Amazon count is asynchronously fetched,
and the component error prevented it from processing.

So bottom line, for end-users again the behavior when errors happen is puzzling.
The page definitely looks off, but there's no direct message that an error occurred.

More than anything else, we want errors like this reported to Bugsnag so we
can address it and prevent it from happening in the future. But is there anything more to do to create a higher quality experience?

A heavy-handed approach would be to handle client-side errors like this by fully redirecting to an error page.
Honestly that would be a decent approach, because it makes the fact that there's a problem very obvious.
A more finesse approach would be to display an error message on the same page.
Although as you can see in this example, the effects of an error on the page rendering can be unpredictable.
That's why I am more confident that the full redirect-on-error approach will work for any arbitrary unhandled error.
It's a little severe, but again for an unexpected error our strategy devolves to communication,
so being loud and clear seems OK.

NS

I want to go back to the server-side for a moment and discuss
the tactic that our DelayedJob extension takes, where it handles errors at the edges.
It's at the entrypoint to the program, at the top-level of executing a given background job.
That's a starting point for some business domain action. That makes it a good seam for communicating errors.

If the backend handles all errors at the boundary of the action,
it provides a natural subject to communicate about to the end user.
If it failed, then no matter why it failed,
whatever deep internal code of the application where the problem occurred,
as a default communication strategy we can just say reference the top-level domain action that prompted the error.

Like if we're executing a background job that publishes Enhanced Content, then at the entry point to that job
we know that context, so the simple strategy is to handle all errors there and for the end-user, just say
"This attempt to publish your content failed."

That strategy also works nicely for REST APIs because each URL is an entry point for a domain action.
So the web controller can simply say something like "Failed to copy your Enhanced Content layout," regardless of the lower-level
details that caused the error.

NS
But notice it doesn't quite work that neatly for GraphQL because we don't have server-defined
seams like that, since the GraphQL endpoint is processing arbitrary queries.

Build Out 1
In this example query,
suppose one of the GraphQL field resolvers raises an exception.

Build Out 2
Thanks to SalsifyGraphQLServer's BaseController, we'll package this into a GraphQL errors response.
It will set the returned message to "Internal server error".
So if we display that message in the client it won't
communicate anything about specific domain action that prompted the error. That's really relevant for SPAs because the client
is often firing off multiple requests, it's not one-to-one with
the user's clicks.

NS
So one way to try to confront that is to view each GraphQL field as an entry point.

Build Out 1
In this version I'm rescuing errors for the field-level resolver and turning it into a user-oriented error message.

Build Out 2
And then the response provides a message that the client could display.
But you can see that adding error handlers at this level would add a ton of extra code.
Plus, the error messages we could set on individual fields still often wouldn't be communicating about
the overall domain action that failed.

So for GraphQL requests I'd want re-frame the understanding of what the entrypoint is, and say it's actually
the client side request. The client will have context on the domain action it's doing when it submits that
GraphQL query, so if the server errors, the client can display a message referencing that domain action.
And then if the client wants to add in the message field returned by the server to provide more detail, sure.
But the key point is we've defined the entry point in a way that supports a reasonable communication to the end user.

OK. So then for errors on the client side itself we have a similar challenge
because the client application has so many entry points.
Every attribute in every nested component and every component action
are independent entry points that can error.

Catching exceptions individually at all of those entry points would be a lot of effort and
still wouldn't always hit the right level of detail to communicate to end users.

And again, I'd suggest re-defining the entrypoint, and zoom out to those built-in browser events for unhandled errors.
They are a good way to create a backstop to ensure we have communication in place for any error,
instead of catching exceptions on every single component attribute.

NS
Because the fundamental idea here is that when a runtime exception happens,
to achieve a good quality user experience we need to communicate it,
but moreover, communicating it is usually the only reliable thing we can do.

I see this as a key insight, because historically most programming languages signal errors in a way
that blurs together different kinds of issues, so it's too easy to lose track of this point,
that either the program knows in advance how to deal with this issue as part of the its domain,
or it doesn't, so its only recourse is to report the issue to humans.

That drives this design of pushing error handling to the boundaries.
Because if we understand exceptions this way, then most of the time
there's no point messing around with error handling at all in the core of our programs.

But this approach is different than a lot of the code I've both read and written myself in the past,
where error handling logic is strewn throughout the codebase.
You see some try-catches on some functions but not all,
some modules have custom Error classes defined but not all,
some of the files have rolled their own version of a Result type.
And those tactics may all make sense in their local context
but they don't go all the way to providing a good experience for end users.

The antidote to that is to make sure we deal with exceptions adequately at the boundaries.
Because then we can have the confidence to not put error handling code at lower levels at all.
Just communicate that it happened, look at Bugsnag, and when it makes sense,
adjust the code to prevent that error condition from happening in the first place.

Establishing a solid pattern for communicating the problem makes a huge difference in quality.
Partly because it reduces confusion for end users and instills more confidence in the software,
and also because the increased visibility makes it more likely that
we'll get around to addressing the underlying bugs.

I want to admit that it's challenging to implement these practices,
but I think the handle-at-the-boundary approach is a good mental framework to make it more actionable.
Because it naturally implies a checklist, where each time we add a new feature,
that's the time to ask, what is the entry point here,
and how will it communicate errors to end-users?

Ultimately,
in addition to creating a much better user experience,
it simplifies development overall.

OK that's it, thank you, and I'm happy to take any questions or comments.

