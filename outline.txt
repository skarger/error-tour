
1. Memorize your introduction and conclusion.
2. It’s not about you.
3. Do everything you can to help them hear and understand you. People are bad at listening.
    Use short words, sentences and paragraphs to express your ideas;
    physical, concrete and vivid images that appeal to the senses;
    and active verb choices in place of abstract or passive language.
4. Don’t drown your audience in data.
5. Eliminate anything that doesn’t clearly support your purpose.
    If a slide, statistic, joke, or anecdote doesn’t serve your goal, cut it.
6. Record yourself or practice in front of real people — or both, if you can.


Hi everyone, I'm Stephen Karger, thank you for attending today.

OK, before I get into the specific topic this talk is all about
I want start out with a bigger question.

When it comes to software, what is quality?

What does it actually mean for software to be high quality?

If you think you've built quality software in the past, what did you do?
What details made it quality?

So I've come up with an answer that I like,
which is that quality software has the following traits:

Build Out 1

* First, does the software provides a nice user interface?
    Does it look good, is it intuitive, does it make it easy to do what you need to do?
* Another trait is performance. If it's responsive and fast then it's higher quality.
* Next, correctness. I like to describe this as if you add 2 + 2 you want to get 4.
    And I would admit, yes in the real-world we don't *always* have a single clear definition
    of the correct behavior.
    Still, one of the hallmarks of good quality software is that it's gone to the trouble of sorting out
    a definition of correctness for complicated situations.
* And a final quality trait, does the software handle errors gracefully?
    Notice that error handling is a slightly different issue than correctness.
    Partly because even if your code is perfectly correct, some errors will happen outside of it,
    like say a transient network error. But even in cases when an error happens
    because your code is logically incorrect, the software overall may still handle that error well,
    and that makes a real difference in the user's experience.


Build Out 2
One reason I like this definition is that it mostly isolates quality traits
from domain-specific features.
There is some overlap, like you'll hear people say that performance is a feature,
and I truly see that point, but still that's
different from what we usually understand as domain features.

Also, this helps shed light on why software quality is challenging to attain,
because it is domain features that our business stakeholders usually request from us.
Features are the front line of making sales.
We have PMs and eng management putting a lot of coordinated effort into planning out feature roadmap.
Whereas pulling off quality requires a lot of initiative from individual engineers.

But I will call out that I noticed an interesting point
in Jason's presentation about our Series E funding, that the investor research had shown that
customers viewed Salsify as a good quality solution that they were more likely to recommend,
and that made the investment more attractive. So quality does translate into real money.

Anyway, all of that is to give you the background ideas behind this talk,
because the rest of it is lower level.
This talk is a deep dive on that last aspect of quality, error handling.

Something I've noticed across my software engineering life is that it's really common
to deal with error handling case-by-case when we have to, and basically invent ad-hoc solutions.
And as a result it's always a little messy.
But that means being a little more deliberate with error handling is a clear path towards higher quality software development.

Now let me say I expect that people will already be at least somewhat familiar with the ideas I'll cover here.
But still my hope is that you'll find it interesting and thought-provoking to see this material presented all together.


NS
OK then. Here is the Enhanced Content UI in Salsify.
I've opened this publish dropdown in the top-right and it's working as intended,
showing me a collection of retailers where I can send my Enhanced Content.

NS
But in the next slide, this is a real bug that we had.
I click the Publish dropdown, and I just see this empty box.

By the way this bug was never reported to our dev team.
I only found it because I'd noticed it in Bugsnag a few times and investigated.

Well if I open the developer tools I can see that the API request
to load the retailers returned a 503, and that's why it couldn't populate the list.

But imagine the end-user's experience here. They see this empty box.
It's confusing, it's not truly evident that an error occurred.
A user might think they missed some setup step, or maybe
they think they just don't understand how the app works.

NS
Another example on this next slide, I've just used that panel on the right-hand side to
update my Enhanced Content layout to reference an Image property.

If I open dev tools though I can see that when I set that value,
the client tried to save the layout, but the API returned a 500 error.
So even though the UI looks fine, if I happened to reload the page
I'd discover that it didn't save my work.

And I want to emphasize that for end users the experience of errors is very frequently like this.
The software behaves in a way that seems off, but it's subtle. It's hard to understand cause and effect.
Ultimately that leads to lower confidence in the software.

NS
So then what do we want?
Build out 1
we don't want them to happen in the first place!
Clearly, preventing errors is preferable to dealing with them at runtime.

And we employ several techniques to try to prevent errors.
Build out 2
Testing, type checking, and code review are all helpful for preventing errors.

NS
But we all know our software still has errors in production.
Over time we learn about bugs and we fix them,
say by supporting a case we hadn't known about before,
or telling the user to correct their input,
or adding a null check before we try to execute some logic.

And this process of widening the range of states that the program can handle,
so that errors become non-errors,
is one of the core tasks in the day-to-day of maintaining software.

But I want to point out that these techniques I've just mentioned are before-the-fact and after-the-fact.
When we write tests, we're preventing errors we anticipate from occurring in the first place.
We react to runtime errors that have happened by updating the code so they won't happen again.
When we apply these techniques we're using knowledge that we already have.

Importantly though, there's also category of errors that we don't know about concretely.
Our application has a code path where an error can happen,
but we're not aware of that right now,
or sometimes we are aware but we assume it's very low probability.

That is, a guiding idea here is that error handling design needs to confront uncertainty.
When we think about how to handle runtime errors, we're designing for situations
that we don't have full knowledge about at this moment.

So in the remainder of this talk I'm going to be talking about code,
but I hope you'll keep this main idea in mind:

That error handling concerns situations that we do not fully anticipate,
and it follows that the first principle behind error design is communication. A good error design needs to provide guidance for
    how do we inform end users that something went wrong.
    but also:
        how do we inform customer support,
        how do we inform PMs,
        and how do we inform ourselves?


NS
Now I want to talk about history.

The C language emerged in 1972 and it influenced a lot of what happened later.

NS
In C error handling conventionally happened via return values.

In this example C program we're making a system call to open a file, and that returns a file pointer.
Then we're trying to write to the file using that file pointer.
Build Out
But the file doesn't exist. Writing to that non-existent file results in a segfault
and the program is killed by the operating system.

NS
So in the next slide I've fixed that bug.
Now I have logic to only write to the file if the returned file pointer is not NULL.
Build Out
The program continues without crashing.

This works. But there are two problems. First, it's mashing the error signaling into the return value.
The function is supposed to return a file pointer but it might
give us something we cannot use as a file pointer.
Second, more importantly, in C nothing forces you check that return value.
You can keep barreling right on.
And failing to check return values for errors is a very common cause of bugs,
and those bugs can be confusing, because they may manifest somewhere far downstream
of the original problem and cause the program to suddenly crash,
or write corrupt data, or have a security vulnerability.

NS
So then there was a stretch in the 80s and 90s where some new languages came out that
have become mainstream in commercial software development.
These languages here all brought a concept of Exceptions.

NS
Starting with this example in Java, I want to parse some strings into integers.
It works for the string 1000 but then throws an exception for the string A grand.
And this shows the general form of Exceptions:
Try running some code, if any of the statements fail,
then immediately jump to the catch block, and execute that other code instead.

NS
Next we see an example in Ruby, same exact concept, just using the words begin rescue ensure
instead of try catch finally.
<pause>

NS
And then we have a JavaScript example. An interesting point here, unlike Java,
JavaScript's parseInt function does not throw an exception when it fails. Instead it returns the value NaN.
I'll say more about that design later.
But for the sake of the example I had to check for NaN and then explicitly threw an exception.
JavaScript does support this same try-catch-finally construct with the error conveyed as an Exception.

So Exceptions were a way of preventing the problem in C where the programmer forgets to check return values.
Exceptions forcibly re-route the code execution.

NS
But even though that's helpful it has a dark side.
It introduces implicit control flow, sort of like a GOTO statement, where they make code abruptly jump.
This can make program logic hard to reason about, because when you are reading the exception handling code,
it's not always obvious what the context is. How did we get here? What are all the circumstances where
we'd be running this handler code and what details do we need to clean up?
Will we even have the references in scope that we would need to clean up?

NS
And some software engineers out there view the problems with Exceptions as unacceptable.
Recent languages that have become mainstream decided not to have exceptions at all.
They went back to communicating errors with return values, like C, just with some twists.

NS
In Go, the big change compared to C is that functions return two distinct values,
with the error value in the second position,
so return values and errors are not overloaded.
And ergonomically this is much better. But you are still free to ignore the error value.

NS
In Rust, they moved the problem into the type system.
The standard approach for functions that can fail is to wrap the return value in a Result type.
So a Result data type has two variants, Ok or Err,
and the Rust type checker forces you to cover those possibilities.
So Rust doesn't have Exceptions, but the compiler prevents you from forgetting to handle errors.

This style of wrapping a return value in Result type supports a functional style of programming,
because instead of throwing an exception as this side effect of your function call,
you're just passing data in and getting data out.

NS
I'll show a more involved example of that next.
On this slide there are these three helper functions, parse_number, ensure_positive, and double.
parse_number tries to convert a string to an integer and returns a Result type, where the Err type is a ParseIntError,
ensure_positive checks if an integer is greater than 0 and returns a Result type, where the Err value is a String,
and double just multiplies a number by 2 and returns a plain number.
<pause>

NS
So then in the next slide I have a full program that uses these functions.
main calls the two_track_computation function and passes it different strings.
The reason I've called it two_track_computation will be clear in a moment.

Looking at two_track_computation, it starts out by calling parse_number,
and the Result value flows into map_err which just formats the error,
and then it flows into the ensure_positive function, then it tries to
double the result, and finally it produces a string value depending on
whether the Result was Ok or an Error.

The details of Rust aren't important here, the point is to see the pattern.
The data is flowing in a linear way from function to function,
with errors at any point conveyed inside these values of data type Result.
There are no Exceptions that cause the control flow to jump. Data-in, Data-out.

MS
This style is not unique to Rust,
in 2014 an F# programmer named Scott Wlaschin
wrote a blog post naming this style Railway Oriented Programming.
It's this style where data flows through your program
like it's on a railroad track, and as long as it's successful it keeps rolling along that success track,
but if it encounters an error it switches to the error track.


NS
So you can string together a functional pipeline this way.
And this can be easier to follow than the control flow jumps you get with exceptions.

It's worth noting that we can adopt a Railway Oriented style in Ruby and JavaScript,
where we represent errors as data that we return from functions.
And if you use Sorbet or TypeScript you could even enforce
that the code handles the success and error variants.

NS
But there are some tradeoffs to consider about that. I'll mention a few of them.

One place where representing errors-as-data fits well is for errors that are part of the program's domain.
If encountering this error is a normal event that you expect to happen sometimes as this program runs,
then it's nice to process that error as plain old data, rather than using exceptions as a special construct.

That's really a way of re-stating what I mentioned earlier that a core role of
error handling is dealing with situations we do not expect. But now I'll make that more
precise by saying that it's actually Exceptions that are for dealing with situations we do not expect.
When something happens that really is outside the circumstances this program is intended to support,
exceptions are a nice way to just bail out immediately.

It can be fuzzy to say whether something is a normal expected problem or if it's exceptional.
An example of that which comes up at Salsify is when we make HTTP requests
between our backend services. It's well within the domain of HTTP in general
that you can receive a 404 or a 500 response, they're not really exceptional.
But when we're making inter-service requests,
getting those response statuses is usually not part of our application's domain,
because the HTTP request is just an implementation detail within a service architecture.
If we don't get a 200-level response it means something has gone wrong.
If the error response is a transient network error then we can retry,
but otherwise the application just has to bail out at that point.
So raising an exception when we see that 404 response actually is a solid strategy.

NS
Then another tradeoff to consider with errors-as-data is loose coupling.
Thinking again of a web API, when it needs to return an error, how do we do it?
In that case we're obligated to put the error on the wire.
We need to transmit it to a different computer running a different program.
So we send the error as data, maybe an HTTP response with status code 500 or 404,
or maybe we send a JSON payload with an errors field.

When we have two programs communicating remotely,
there's this imperative to make them work with looser coupling.

But even inside one program it may be useful to apply that lesson from web APIs,
and pass pure data around in our program to promote modularity.
If we want to have a clear boundary in our program, we can ask:
Will this work if I only can pass data between these two sub-systems  of the program?
If I were to extract these two parts into separate programs,
would it work to communicate over a web API or a message queue?
And if so, how would they signal error situations to each other?

If it's desirable for those pieces to be loosely coupled,
then a step towards that is to communicate errors as data between those parts of the program.

NS
Another practical tradeoff between exceptions and errors-as-data is
Are you working in a platform that has exceptions?

So Clojure is a language designed to run on the JVM and have seamless interop with Java.

And with TypeScript, yes we can use the type-checker to support static enforcement of Result types
in a similar style to Rust, but TypeScript is meant to integrate directly into JavaScript.

NS
You can see in this Clojure example it's calling Java to try to parse the string to an integer,
it calls the Java parseInt function to do that,
and it throws the same Java exception as in the earlier example.

So even though Clojure is functional language that puts heavy emphasis on passing data
through your program, as a practical matter Java throws exceptions so Clojure has to work with that.

And JavaScript is the same way, even using Typescript you need to deal with exceptions anyway.
That may push the decision more in favor of choosing exceptions as the strategy for how to signal errors.


NS
Lastly another very good reason to use exceptions is that it's a way
to make them visible to us. With exceptions, we have infrastructure in place to report them to Bugsnag.

A key challenge for us as developers is that errors are hard to see.
Hopefully they only happen infrequently, and they often only come out with real-world usage.
So it's easy discount their importance.

Sometimes we'll realize there's a potential for an error, but we dismiss it as an "edge case".

NS
Calling something an edge case is like a geometric idea. You have this square representing all the cases,
and most are the yellow in the middle there,
but there's a thin edge of cases around the outside that we're not covering.

But you need error reporting to understand the size of the edge.

NS
Is it more like this? And if so we really we ought to deal with it.

NS
In one of my previous jobs we had a situation where most of the time our cases looked like this,
but on the two days of the year when the times changed for daylight savings time,

NS
the edge was like this and it was a disaster.
But then the next day would arrive and it would become hard to prioritize fixing it.

You really need records of errors in order to prioritize them,
and pragmatically reporting exceptions to Bugsnag is a way to accomplish that.

NS
For the last portion of this talk I want to get into some more Salsify-specific details.
At the beginning I was talking about doing the more deliberate design for error handling,
but the truth is we already do that to some degree in our shared libraries.

Starting out on the backend, here's a snippet from the SalsifyGraphQLServer ruby gem
which I've shortened to show on the slide.
For every GraphQL request, this BaseController will catch errors and use our ExceptionReporter gem to send the error to Bugsnag,
then finally it translates the error into an API response following GraphQL conventions for an error payload.

There is more to it than that, in reality it handles more distinct errors, and also reports the error to New Relic.
(pause to look at code)

NS
For background jobs, Salsify has an extension to Delayed Job.
It has a similar approach of catching errors and reporting them to Bugsnag.
(pause to look at code)

So if you're using these libraries then you automatically have this basic error handling framework.
And this is a very helpful default, but by itself it's not enough to attain high quality error handling.

What I've shown so far is backend only. But once we translate the error into an API response payload, then what?
Or in the case of a background job, once we update a database row to record that this action failed, then what?

The backend can't be the end of the story. We still need to communicate the error to the end user.

Making that final step, connecting the dots on the client side, really improves quality a lot.
That may be an obvious point but in real world development it's easy for it to fall through the cracks.

NS
Now just as with the backend libraries, we already do have some built-in error handing within the main Salsify client app.

What the frontend error handler service provides is when an Ember route tries to load data from the API,
but the API request returns an error status like a 404 or 500,
the client will flash an error message in a bar near the top of the screen.
It's a bit generic but at least it's providing feedback to the end-user that something went wrong.

But remember those Enhanced Content examples I showed at the beginning where the API returned errors
but the end user had no signal of that. Why didn't we see that warning bar?

Let me show you another real example. This screen comes from the Enhanced Content Ember engine.

In this video you'll see I'm looking at my list of Enhanced Content layouts.
Then you'll see I'm going click one of the layout rows and then click the button to copy it.
But under the hood, the API request to load that layout is going to fail. You'll see that below in my network tab.

Let's watch...

So click, copy, nothing happens, no feedback.

So, in theory Salsify has this standard UI feedback for failed API requests,
but in practice it's not always working.

Now the reason this isn't working here is bit tricky.
The problem is that the scope for the error message is set to the copy modal, but that modal isn't visible when the error occurs.
That's why you don't see the flash message.

On one hand you could blame the EC engine for this, because it has an error in its error handler.
But when we're working on an engine like this, in a normal development workflow we can't see that error behavior,
because it's provided by the main app in dandelion. The only way to see it is to package the engine into dandelion
and test end-to-end, including forcing the backend API server to return a 500.
The fact is that problem has existed for awhile but I only discovered it because I was preparing this tech talk.

Also with our current setup, it's difficult to write an automated test
in the Enhanced Content engine that realistically exercises the UI behavior for a failed API request,
again because the error message bar lives in dandelion.

This is a place where the micro-frontends initiative should help, because a micro-app will
be able to manage more of its own UI, including its error display, so we won't have the same pitfalls as with an engine.

Still though, I'd say the current dandelion client has the right overall idea to have a default
behavior where if an API request fails, let's at least provide some visual feedback.

Because it ultimately doesn't matter whether the server indicates errors in a well-formatted way,
if we don't carry that through end-to-end so that the client communicates the problem to the user.

By the way in these examples I focused on synchronous web requests but communication of background job failures
might be even more important for a lot of Salsify features.

As I showed before we have the backend error reporting extension for DelayedJob,
but for actually communicating background job failures to end users,
that's an example where the status quo is to figure it out ad hoc.
Usually, each time we add a new background job, the approach for handling failures isn't covered.
Then over time we see failures in production and it's like OK now we need to devise some UX to handle this case.
And we're starting out with basic questions like, what even screen do we show the error on? Or instead of showing it in the client, should we send an email?

So one actionable takeaway here is every time we add a new background job, consider how to communicate when it fails, and then verify that it works.
Similarly, for any API request the client makes, a good baseline for failures is verify that it flashes a generic error message bar in the style of the main dandelion client.

NS
Beyond server errors, I want to cover errors in the client itself,
for example in the logic of an Ember component.

NS
In this slide, I have a catch block set up that handles when I call badFunc.

Build Out 1

But there's no catch block lower when I call anotherBadFunc.

Build Out 2

When a JS runtime error happens but it's not handled by any catch block,
the browser will fire the window.onerror event.
That can be configured to run a function as a last resort error handler.


NS
For rejected promises, similar idea but slightly different.

Build Out
Here I have a promise that will reject because it tries to parse HTML as JSON.
But in this code there's no catch chained onto the promise.

NS
In this case the browser will fire the unhandledrejection event.

Build Out
Like window.onerror, this can be configured as a last resort handler for rejected promises.

In fact, the basic way to configure Bugsnag on the browser side is to set up window.onerror and unhandledrejection
to report errors to Bugsnag.

NS
In our case though, Ember provides a wrapper function, Ember.onerror, that's meant as the backstop for unhandled errors in Ember apps.

Build Out
And then Salsify's error-reporting library abstracts over Ember.onerror.
Ultimately, the error-reporting library configures reporting to Bugsnag.
That happens once in Dandelion. That one-time setup then applies to our mounted engines.

But it does mean that in your engine you can't use these top-level error events itself.
In particular you can't use them as a way to communicate problems to end users, in addition to the Bugsnag reporting.
This is another place where the micro-frontends project will enable improvements.

NS
Now let me make this more visual with an example of a client error.
Here we're looking at the list of Enhanced Content layouts again.

Notice the rows of the table. They're Ember components.

Build Out
So we have a component representing the table row, and it includes some of the row data itself

Build Out
but then inside it we have a component that displays what retailers you've published this layout to,
Build Out
and another component that shows the status of the layout.

So imagine that that "Most Recent Status" component throws an exception, what's going to happen?

NS
Well the overall layout table still renders, but because the status component errored, everything
after that is gone. You can see there's no status, but also the rows after the first one aren't showing.

Build Out
Also, unexpectedly the error thrown by that layout status component also causes
this Amazon quota bar up at the top to break as well, where now it's not showing counts anymore.
That's a random side effect of the fact that that the Amazon count is asynchronously fetched,
and the layout status component errored before it was processed.

So bottom line, again for end-users the behavior when errors happen is puzzling.
The page definitely looks off, but there's no direct message that an error occurred.

So more than anything else, we want errors this reported to Bugsnag so we
can address it and prevent it from happening in the future. But is there anything more to do?

A heavy-handed approach would be to handle client-side errors like this by fully redirecting to an error page.
Honestly that would be a decent approach, because it makes the fact that there's a problem very obvious.
A more finesse approach would be to display a message on the same page that an error occurred.
Although as you can see in this example, the effects of an error on the page rendering can be unpredictable.
That's why I am more confident that the full redirect-on-error approach will work for any arbitrary unhandled error.
It's a little severe, but again for an unexpected error our strategy devolves to communication,
so being loud and clear seems OK.

NS

I want to go back to the server-side for a moment and discuss
the tactic that SalsifyGraphQL and DelayedJob extensions take, where they handle errors at the edges.
It's at the entrypoints to the program, where an API request comes in, or at the top-level of executing a background job.
These are starting points for some business domain action. They're good seams for communicating errors.

If the backend handles all errors at the boundary of the action,
then that provides a natural subject to communicate about to the end user.
If it failed, then no matter why it failed,
whatever deep internal code of the application where the problem occurred,
as a default we can just say something like "Failed to load your Enhanced Content layout," or
"This attempt to publish your content failed."

But it's a little more complicated on the client side because we have so many entry points.
Every client side route is an entry point, but so is every nested component, and every component action.

So one hand I do think it's possible to pursue same pattern on the client side, where we handle errors
individually at all of those entry points, but it's a lot of effort.
Conversely, those top-level browser events for unhandled errors are a more coarse grained way to handle errors,
but it's a good default. It's a good way to create a backstop so we have a baseline communication in place
for errors, and then put most of our effort into preventing the errors from re-occurring.

NS
OK quick detour. There's a great talk by Gary Bernhardt called Boundaries, where he
talks about a couple common problems that arise in automated test suites.
One problem is that often the tests are slow, for example because we're reading and writing to the database a lot,
or spinning up a web browser to verify the rendered UI. But then the opposite problem for is over-using
test mocks. Even though mocking can help the tests go faster, it often leads to brittle tests that
essentially just repeat your implementation, rather than actually verifying the intended behavior.

And so the solution he articulates is what he calls Functional Core, Imperative Shell. The idea is
that you write most of your code in a functional style, where you're just passing data to functions
and getting data back. Most of your code is restricted to domain logic.

And only at the edges of the program do you deal with the messy,
side-effecty things like the database and external APIs.
And then for your tests, for the majority of code, you won't even need to bother with mocks,
you can safely execute the production code,
and mostly it will go fast since you're not doing any side-effects.

Well we can apply a variation on this idea for thrown exceptions.
Now I admit I'm stretching the metaphor because exceptions are a side effect themselves,
so by definition we're no longer talking about pure functional programming.

Still though, we can translate the idea, and push error handling to the boundaries.
Then in the core of our programs we don't mess around with error handling at all.

This approach is different that a lot of the code I've read and code I've written myself in real-life,
which has error handling logic strewn throughout the codebase.

And granted, one reason for that is sometimes we have to deal with functions that throw exceptions
for cases that aren't really exceptional, where really it would be better
if you just got some domain-specific return value but instead you have to catch an exception.

But the other big reason why real-world code often has error handling all over the place comes back to
the common situation that a project doesn't have a clear overall error design.

But if we create confidence that the app will deal with exceptions adequately at the boundaries,
then we wouldn't feel the need to throw in ad hoc handling at lower levels.

So to wrap up:
Historically, the way that programming languages signal errors blurs together different issues.

There are problems that we expect and they're actually just an infrequent but normal situation in our domain,
and we can write code to deal with that situation. At that point from our program's point of view
it's no longer really an error, it's just another case that this program covers.

Then there are problems where we know in advance that they will happen, but we decide that do not want to expand our
program's domain to cover them. Again a common example of that is when our backend services
make HTTP requests to each other, and we know we will get responses with error status codes sometimes,
but that's an implementation detail. There isn't a nice way to integrate that into our program's domain model,
so we call it an exception and bail out.
And third there are unexpected problems, like logical bugs in our code, or running out of memory.
We do know that errors like this will happen, but we don't know exactly where ahead of time.

And it's worth noting that a given error can move across these categories over time.

Ultimately the crux of this talk is about those errors that are outside our domain,
whether we expect them ahead of time or not.
The key insight is that we our only real recourse for these is communication.
That's challenging to pull off because it's not totally clear who is responsible.
Devising the approach takes collaboration between engineering, product, and UX.
But taking that extra step makes a huge difference in quality.

And finally, a helpful design framework is to build error handling into the boundaries of our programs.

It's easy to preach best practices in a tech talk but day-to-day when we're focused on feature development
it's hard to grow the scope of any given Jira ticket in order to sort out error handling.
So a way to make this stuff actionable is set up error handling at the boundaries.
Then in the bulk of programs we can ignore errors because we're confident that when they happen,
we will adequately communicate to users, and also give ourselves enough information to make any necessary updates.

OK that's it, thank you.

